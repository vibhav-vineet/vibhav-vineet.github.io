
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN" "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">

<head>
<link rel="shortcut icon" href="myIcon.ico">
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />

<meta name="keywords" content="Vibhav Vineet"> 
<meta name="google-site-verification" content="yy_3iiS_X6pJdegdwitJMrH0LRLHXwpjrV9RKLXxKjg" />
<link rel="stylesheet" href="jemdoc.css" type="text/css">
<title>Vibhav Vineet</title>


<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
  ga('create', 'UA-87320911-1', 'auto');
  ga('send', 'pageview');
</script>
</head>

<body>

<nav class="navbar navbar-dark navbar-expand-lg fixed-top">
    <div id="layout-menu">
        <a href="#publication">Home</a>
        <a href="#publication">Publication</a>
        <a href="#experience">Collaborators</a>
    </div>
</nav>

<div id="layout-content" style="margin-top:25px">
<table>
    <tbody>
        <tr>
            <td width="670">
                <div id="toptitle">
                <!--<h1>Hengshuang Zhao &nbsp; <img src="pics/name_chs.png" height="40px" style="margin-bottom:-10px"></h1>-->
                <h1>Vibhav Vineet</h1>
        </div>
        <p>
          Senior Researcher</br>
          Microsoft Research</br>
          Redmond, WA</br>
          Email: firstname[dot]lastname[at]microsoft[dot]com</br>
        
        </p>
        <p>
          <a href="http://scholar.google.com/citations?user=E_UlAVQAAAAJ&hl=en">Google Scholar </a> , &nbsp
          <a href="http://dblp.uni-trier.de/pers/hd/v/Vineet:Vibhav"> DBLP </a>, &nbsp
          <a href="https://www.facebook.com/?_rdr=p"> fb </a>
        </p>
      </td>
      <!--<td><img src="pics/hszhao.jpg" border="0" width="250"></br></td>-->
      <!--<td><img src="pics/hszhao.png" border="0" height="250"></br></td>-->
      <!--<td><img src="pics/hengshuangzhao.png" border="0" height="250"></br></td>-->
      <td><img src="images/vibhav.png" border="0" height="250"></br></td>
    <tr>
  </tbody>
</table>

<!--<h2>Biography [<a href="./hengshuangzhao.pdf">CV</a>]</h2>-->
<h2>Research</h2>
<p>
  <div style="text-align:justify">My current research interests are in computer vision, machine learning, robotics. More specifically I am interested in developing algorithms to solve generalization issues in computer vision and machine learning such that these models can be deployed in the real world environments. I also work on using synthetic data and neural renderer to solve computer vision and robotics problems. 

<br> <br>

<em> <strong>If you are interested in research collaborations or doing research internship at MSR Redmond, please contact me. </strong> </em>

  </div>
</p>
<!--
<p>
  <div style="text-align:justify">Previously, I have spent wonderful times as a postdoctoral researcher at <a href="https://torrvision.com">Torr Vision Group</a> in the Department of Engineering Science at the <a href="http://www.ox.ac.uk">University of Oxford</a> (beautiful Oxford), working with <a href="http://www.robots.ox.ac.uk/~phst">Prof. Philip Torr</a>. I obtained my Ph.D. degree in the Department of Computer Science and Engineering at <a href="http://www.cuhk.edu.hk">The Chinese University of Hong Kong</a>, supervised by <a href="http://www.cse.cuhk.edu.hk/~leojia">Prof. Jiaya Jia</a>. During Ph.D., I have spent wonderful times as an intern with <a href="http://xiaohuishen.github.io">Dr. Xiaohui Shen</a>, <a href="http://sites.google.com/site/zhelin625">Dr. Zhe Lin</a>, <a href="http://www.kalyans.org">Dr. Kalyan Sunkavalli</a>, <a href="http://www.brianpricephd.com">Dr. Brian Price</a> at Adobe (San Jose), <a href="http://www.cs.toronto.edu/~urtasun">Prof. Raquel Urtasun</a> at Uber (Toronto), and <a href="http://vladlen.info">Dr. Vladlen Koltun</a> at Intel (Santa Clara).</div>
</p>

<p>
  <div style="text-align:justify">My general research interests cover the broad area of computer vision, machine learning and artificial intelligence, with special emphasis on building intelligent visual systems. My research goal is to utilize artificial intelligence techniques to make machines perceive, understand and interact with the surrounding environment, and ultimately make high positive impacts on various fields.</div>
</p>

<p>
  <div style="text-align:justify">Several specific topics of our current research interests and focus: 1. image/video recognition like classification, segmentation, and detection; 2. 3d point cloud processing, scene reconstruction, and manipulation; 3. representation learning, unsupervised learning, weakly-/semi-supervised learning; 4. open-world learning, transfer learning, unified systems, advanced architecture design; 5. autonomous driving, multi-modal learning, vision+language, pretraining; 6. embodied ai, social navigation, interactive navigation, robot learning, etc.</div>
</p>

<p>
<div style="text-align:justify"><font color="red">Multiple openings for self-motivated PhD, Postdoc, RA and Interns! The current round for <a href="https://www.cs.hku.hk/programmes/research-based/admission-2023">PhD application</a> is the <a href="https://i.cs.hku.hk/~gradappl/index.html">Early Recruitment Scheme</a>, which ends on <b>May 1st, 2022</b>. The deadline for the <a href="https://www.cs.hku.hk/rintern">Research Internship Programme</a> is <b>May 31st, 2022</b>. If you are interested in working with me, please drop me an email with your resume ASAP. Remote collaboration is also welcome!</font></div>
</p>
-->

<!--
<p>
  <div style="text-align:justify">I am currently a postdoctoral researcher at <a href="https://www.csail.mit.edu"> Computer Science and Artificial Intelligence Laboratory (CSAIL) </a> at <a href="https://www.mit.edu">MIT</a>, working with <a href="http://mit.edu/torralba">Prof. Antonio Torralba</a>. Before that, I have spent wonderful times as a postdoctoral researcher at <a href="https://torrvision.com">Torr Vision Group</a> in the <a href="http://eng.ox.ac.uk">Department of Engineering Science </a> at the <a href="http://www.ox.ac.uk">University of Oxford</a> (beautiful Oxford), working with <a href="http://www.robots.ox.ac.uk/~phst">Prof. Philip Torr</a>. I obtained my Ph.D. degree in the <a href="http://www.cse.cuhk.edu.hk">Department of Computer Science and Engineering</a> at <a href="http://www.cuhk.edu.hk">The Chinese University of Hong Kong</a>, supervised by <a href="http://www.cse.cuhk.edu.hk/~leojia">Prof. Jiaya Jia</a>. During Ph.D., I have spent wonderful times as an intern with <a href="http://xiaohuishen.github.io">Dr. Xiaohui Shen</a>, <a href="http://sites.google.com/site/zhelin625">Dr. Zhe Lin</a>, <a href="http://www.kalyans.org">Dr. Kalyan Sunkavalli</a>, <a href="http://www.brianpricephd.com">Dr. Brian Price</a> at Adobe (San Jose), <a href="http://www.cs.toronto.edu/~urtasun">Prof. Raquel Urtasun</a> at Uber (Toronto), and <a href="http://vladlen.info">Dr. Vladlen Koltun</a> at Intel (Santa Clara).</div>
</p>
-->
<!--<p>My general research interests cover the broad area of computer vision, machine learning, and artificial intelligence, with special emphasis on high-level scene recognition and pixel-level scene understanding. I aim at designing powerful image understanding and point cloud processing frameworks, as well as exploring fancy and interesting raw operators.</p>
<a href="http://shijianping.me">Dr. Jianping Shi</a>, <a href="http://www.ee.cuhk.edu.hk/~xgwang">Prof. Xiaogang Wang</a> at SenseTime (Beijing), 
-->
<!--<p>My general research interests cover the broad area of computer vision, machine learning and artificial intelligence, with a focus on building intelligent visual systems. My research goal is to develop theoretical and practical methods in intelligent visual systems that can ultimately make high positive impacts on various fields. More specifically, I focus on designing elegant and effective neural learning systems, including while not limited to delicate image representation architectures, exquisite point cloud processing frameworks, reliable, generalizable and cutting-edge visual algorithms.</p>-->
<!--<p>If you are interested in my research or would like to work with me, feel free to send me an email.</p>-->

<!--<p><div style="text-align:justify"><font color="red">Update: I will be joining the <a href="https://www.cs.hku.hk">Department of Computer Science</a> at <a href="https://www.hku.hk">The University of Hong Kong</a> as an Assistant Professor. I am looking for self-motivated PhD/Postdoc/RA to work together on computer vision, machine learning and artificial intelligence. In the meantime, our group at the University of Oxford is also looking for self-motivated interns and visitors to work on research projects related to 2D/3D scene recognition and reconstruction. If you are interested in joining my group at HKU or Oxford, please do not hesitate to drop me an email with your resume. Remote collaboration is also welcome. More details about the <a href="./advertisement.pdf"><b>Position</b></a>.</div></font></p>

<p><font color="black", style="margin-left:3.8em">Bottom up 3D instance segmentation <a href="http://github.com/Jia-Research-Lab/PointGroup">PointGroup</a>.</font></p>
<p><font color="black", style="margin-left:3.8em">Backbone structure for 3D scene recognition <a href="https://arxiv.org/abs/2012.09164">Point Transformer</a>.</font></p>

<p><div style="text-align:justify"><font color="red">Update: I will be joining the <a href="https://www.cs.hku.hk">Department of Computer Science</a> at <a href="https://www.hku.hk">The University of Hong Kong</a> as an Assistant Professor. I am looking for self-motivated Ph.D./RA/Interns to work together on computer vision and machine learning. Besides, several Postdoc positions with highly competitive salaries are waiting for talented candidates. In the meantime, our group at the University of Oxford is also looking for self-motivated interns and visitors to work on research projects related to 2D/3D scene recognition and reconstruction. If you are interested in joining my group at HKU or Oxford, please do not hesitate to drop me an email with your resume. Remote collaboration is also welcome.</div></font></p>

<p>
  <div style="text-align:justify">I am also building a research group at the <a href="https://www.cs.hku.hk">Department of Computer Science</a> at <a href="https://www.hku.hk">The University of Hong Kong</a> as an Assistant Professor. I am looking for self-motivated PhD/Postdoc/RA to join my group in Fall 2022, working together on exciting and cutting-edge computer vision, machine learning and artificial intelligence projects.</div>
</p>
-->

<!--
<p><font color="black">Pinned: Highly optimized PyTorch codebases available for semantic segmentation <a href="http://github.com/hszhao/semseg">semseg (PSPNet&PSANet)</a>.</font></p>
<p><font color="black", style="margin-left:3.8em">Unified raw operator for 2D image recognition <a href="http://github.com/hszhao/SAN">SAN</a> and 3D point cloud recognition <a href="https://arxiv.org/abs/2012.09164">PointTransformer</a>.</font></p>
<p><font color="black", style="margin-left:3.8em">Unified panoptic segmentation <a href="http://github.com/uber-research/UPSNet">UPSNet</a> (logit level), and <a href="https://github.com/dvlab-research/PanopticFCN">PanopticFCN</a> (representation level).</font></p>
<p><font color="black", style="margin-left:3.8em">Unified modeling for joint 2D-3D scene recognition <a href="https://github.com/wbhu/BPNet">BPNet</a>.</font></p>
<p><font color="black", style="margin-left:3.8em">Unified tracking framework <a href="https://zhongdao.github.io/UniTrack">UniTrack</a>.</font></p>
-->
<!--
<p><font color="black", style="margin-left:3.8em">Powerful few-shot semantic segmentation <a href="http://github.com/Jia-Research-Lab/PFENet">PFENet</a>, and robust semantic segmentation <a href="https://github.com/dvlab-research/Robust-Semantic-Segmentation">DDCAT</a>.</font></p>
<p><font color="black", style="margin-left:3.8em">Bottom up 3D instance segmentation <a href="http://github.com/Jia-Research-Lab/PointGroup">PointGroup</a>.</font></p>
-->

<div id="News">

<ul>
<li> March, 2022. Two papers accepted to CVPR 2022. </a>
<li> February, 2022. Paper accepted to ACL 2022. </a>
<li> January, 2022. We are organizing workshop on <a href="https://rosecvpr22.github.io/">"ROSE: Robustness in Sequential Data" </a> in CVPR 2022. More details to follow. </a>
<li> January, 2022. Paper accepted to ICLR 2022. </a>
<li> October, 2021. Papers on CausalCity, Taskography, 3DB have been updated. </a>
<li> 4th February, 2021. One paper accepted at ICASSP 2021. </a>
<li> 2nd October, 2020. Neural Prunning paper received the Best Student Paper Award at 3DV 2020. </a>. 
<li> 2nd October, 2020. Two papers accepted to 3DV 2020. </a>. 
<li> 10th September, 2020. IROS 2020 paper has been selected as one of the finalist in the Best Student Paper Award list, <a href="https://www.iros2020.org/">IEEE International Conference on Intelligent Robots and Automation (IROS) 2020 </a>. 
<li> 2nd July 2020. One paper on synthetic data generation accepted to <a href="https://eccv2020.eu/"> ECCV 2020.</a> 
<li> 30th June 2020. Our paper on drone navigation accepted to <a href="https://www.iros2020.org/"> IROS 2020.</a> 
<li> March. 2020. <a href="https://www.microsoft.com/en-us/research/blog/training-deep-control-policies-for-the-real-world/"> Blog </a> and <a href="https://venturebeat.com/2020/03/18/microsoft-researchers-train-ai-in-simulation-to-control-a-real-world-drone/"> media articles </a> on our paper on drone navigation.  </a> 
<li> Feb. 2020. One paper on CNN based acoustic simulation has been accepted in <a href="https://2020.ieeeicassp.org/">ICASSP 2020. </a> 
<li> September 2019: Paper on drone navigation has been uploaded on arXiv. </a> 
<li> March 2019: Paper on large scale dynamic reconstruction uploaded on arXiv. </a> 
<li> February 2019: Papers on photorealistic rendering and privacy preserving computer vision uploaded to arXiv. </a> 
<li> 13th August  2018: Joined Microsoft Research in Redmond. </a> 
<li> 6th February  2017: Started working at FiveAI. </a> 
<li> 12th September 2016: <font color="blue">Media coverage </font> of playing games for data: <a href="https://www.technologyreview.com/s/602317/self-driving-cars-can-learn-a-lot-by-playing-grand-theft-auto/"> MIT Technology Review. </a> 
<li> 24th June 2016: Paper on generating data by playing games accepted to ECCV-16.
<li> 16th March 2016: Two papers accepted to CVPR-16.
<li> 14th September 2015: I joined <font color="blue"><a href="http://vladlen.info/lab/">Intel Visual Computing Lab</a></font>. 
<li> 12th September 2015: My PhD thesis was awarded with the prestigious <font color="blue"><a href="http://www.bmva.org/sullivan">BMVA Sullivan Thesis Prize </a> </font> for 2015. 
The prize is considered for award, on an annual basis, to the best doctoral thesis submitted to a UK University, in the field of computer or natural vision.
<li> 2nd July 2015: <font color="blue">Media coverage </font> of SemanticPaint paper: <a href="http://gizmodo.com/microsofts-latest-quest-into-digital-worlds-may-one-day-1715144372"> gizmodo.com </a>, <a href="http://www.popularmechanics.com/technology/apps/a16264/microsoft-research-semanticpaint/"> popularmechanics </a>, <a href="http://www.fastcodesign.com/3048111/innovation-by-design/love-ms-paint-heres-vr-paint"> fastcodesign.com </a>, <a href="http://news.softpedia.com/news/microsoft-s-semanticpaint-will-digitize-your-life-485795.shtml"> softpedia.com </a>, <a href="http://www.winbeta.org/news/microsoft-research-debuts-another-project-semantic-paint">winbeta.org </a>, <a href="http://www.engadget.com/2015/07/01/microsoft-research-semanticpaint/"> engadget.com </a>
<li> 30th June 2015: <a href="papers/2015/TOG/semanticpaint-TOG15.html"> <font color="blue">Semantic Paint paper </font> </a> for new interactive approach to 3D scene understanding accepted to ACM Transaction of Graphics <font color="blue">(TOG)</font>.
<li> 29 June 2015: <font color="blue">Media coverage </font> of CRF and CNN paper: <a href="https://www.inverse.com/article/4144-play-a-fun-game-of-stump-the-next-gen-photo-recognition-program"> Play a fun game </a>, <a href="https://www.reddit.com/r/InternetIsBeautiful/comments/3bb7mh/deep_learning_image_segmentation_from_oxford/"> Reddit </a>, <a href="http://www.qore.com/noticias/38337/Desarrollan-un-sistema-de-reconocimiento-de-objetos-en-fotografias"> Desarrollan un sistema de reconocimiento de objetos en fotografÃ </a>
<li> 29th June 2015: One paper accepted to IEEE/RSJ International Conference on Intelligent Robots and Systems <font color="blue">(IROS)</font>.
<li> 20th June 2015: I received <a href="http://www.theiet.org/events/2015/211062.cfm"><font color="blue">IET Vision and Imaging PhD thesis award</font></a>.
<li> 25th May 2015: ICRA paper was one of the finalist in the <font color="blue">Best Robotic Vision Paper Award</font>, IEEE Robotics and Automatic Conference (ICRA).
<li> May 2015: Recent work (CRF as RNN arXiv:1502.03240) setting a <font color="blue">new state-of-art in semantic image segmentation</font> on the PASCAL VOC 2010-2012 benchmarks. 2015. 
<li> 20th April 2015: <a href="papers/2015/CHI/SemanticPaintBrush.html"><font color="blue">Semantic Paint Brush </font> </a>paper presented at ACM CHI conference.
<li> 12th Feb 2015: One paper submitted to arxiv on combining <font color="blue">CRF and CNN</font>.
<li> 31th Jan 2015: One paper accepted to ICRA on large scale semantic scene reconstruction.
<li> 24th Jan 2015: Started working as a postdoctoral researchers at <font color="blue">Stanford University</font>.

</ul>

<!--
<h2>Publications [<a href="http://scholar.google.com/citations?user=4uE10I0AAAAJ&hl=en">Google Scholar</a>]</h2>
<ul>
  
  <li>
    <a href="">FocalClick: Towards Practical Interactive Image Segmentation</a><br>
    Xi Chen, Zhiyan Zhao, Yilei Zhang, Manni Duan, Donglian Qi, <b>Hengshuang Zhao</b>.<br>
    <em>Computer Vision and Pattern Recognition</em> (<b>CVPR</b>), 2022.</br>
    <p style="margin-top:3px">
      [<a href="https://arxiv.org/abs/2204.02574">Paper</a>]
      [<a href="https://github.com/XavierCHEN34/ClickSEG">Code</a>]
      [<a href="papers/cvpr22_focalclick_bib.txt">Bib</a>]
    </p>
  </li>
-->




</ul>
</div>


<div id="footer">
  <div id="footer-text"></div>
</div>
  <center>© Vibhav Vineet | Last updated: 07/04/2022</center>

</div>

<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','http://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-88615920-1', 'auto');
  ga('send', 'pageview');

</script>

</div>
</body>
</html>